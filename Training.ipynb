{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load secrets from file\n",
    "import os\n",
    "FP_Secrets = 'Numerai.secrets'\n",
    "\n",
    "if not os.path.exists(FP_Secrets):\n",
    "    raise FileNotFoundError(f\"'{FP_Secrets}' not found. Make sure the file exists.\")\n",
    "\n",
    "# Read API keys \n",
    "api_keys = {}\n",
    "with open(FP_Secrets, 'r') as secrets_file:\n",
    "    for line in secrets_file:\n",
    "        key, value = line.strip().split('=')\n",
    "        api_keys[key] = value\n",
    "\n",
    "# Set your Numerai API credentials\n",
    "PUBLIC_KEY = api_keys.get('PUBLIC_KEY')\n",
    "SECRET_KEY = api_keys.get('SECRET_KEY')\n",
    "\n",
    "if not PUBLIC_KEY or not SECRET_KEY:\n",
    "    raise ValueError(\"API keys not found in the 'numerai.secrets' file.\")\n",
    "\n",
    "import numerapi\n",
    "\n",
    "# Set your Numerai API credentials\n",
    "napi = numerapi.NumerAPI(public_id=PUBLIC_KEY, secret_key=SECRET_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the latest Numerai datasets\n",
    "napi.download_dataset(\"v4.1/train.parquet\", \"train.parquet\")\n",
    "napi.download_dataset(\"v4.1/validation.parquet\", \"validation.parquet\")\n",
    "napi.download_dataset(\"v4.1/live.parquet\", \"live.parquet\")\n",
    "napi.download_dataset(\"v4.1/live_example_preds.parquet\", \"live_example_preds.parquet\")\n",
    "napi.download_dataset(\"v4.1/validation_example_preds.parquet\", \"validation_example_preds.parquet\")\n",
    "napi.download_dataset(\"v4.1/features.json\", \"features.json\")\n",
    "napi.download_dataset(\"v4.1/meta_model.parquet\", \"meta_model.parquet\")\n",
    "\n",
    "# Challenge: How might you use the additional files like 'features.json' and 'meta_model.parquet' in your ML models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pandas DataFrames using `pd.read_parquet`\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_parquet(\"train.parquet\")\n",
    "validation_data = pd.read_parquet(\"validation.parquet\")\n",
    "live_data = pd.read_parquet(\"live.parquet\")\n",
    "live_example_preds = pd.read_parquet(\"live_example_preds.parquet\")\n",
    "validation_example_preds = pd.read_parquet(\"validation_example_preds.parquet\")\n",
    "\n",
    "# Display basic info about the data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", validation_data.shape)\n",
    "print(\"Live data shape:\", live_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializes Numerai Data and NumerAPI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numerapi\n",
    "import re\n",
    "\n",
    "# Set your Numerai API credentials\n",
    "napi = numerapi.NumerAPI(public_id=PUBLIC_KEY, secret_key=SECRET_KEY)\n",
    "\n",
    "# Download the latest Numerai dataset\n",
    "# napi.download_current_dataset(unzip=True)\n",
    "\n",
    "f_pattern = r\"numerai_dataset_\\d+\"\n",
    "f_name = None\n",
    "print(os.listdir())\n",
    "for file in os.listdir():\n",
    "    if re.match(f_pattern, file):\n",
    "        f_name = file\n",
    "        break\n",
    "\n",
    "assert f_name != None\n",
    "f_name = f_name.replace('.zip', '') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads data by chunks (My laptop does not have enough RAM)\n",
    "t_data = os.path.join(f_name, \"numerai_training_data.csv\")\n",
    "tor_data = os.path.join(f_name, \"numerai_tournament_data.csv\")\n",
    "chunk_size = 50000 \n",
    "num_chunks = 10\n",
    "chunks = []\n",
    "for i, chunk in enumerate(pd.read_csv(t_data, chunksize=chunk_size)):\n",
    "    chunks.append(chunk)\n",
    "    if i > num_chunks: break\n",
    "train_data = pd.concat(chunks, axis=0)\n",
    "\n",
    "chunks = []\n",
    "for i, chunk in enumerate(pd.read_csv(tor_data, chunksize=chunk_size)):\n",
    "    chunks.append(chunk)\n",
    "    if i > num_chunks: break\n",
    "tournament_data = pd.concat(chunks, axis=0)\n",
    "\n",
    "# Display basic info about the data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Tournament data shape:\", tournament_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## View Dataset\n",
    "feature_names = [\n",
    "        f for f in train_data.columns if f.startswith(\"feature\")\n",
    "    ]\n",
    "target_names = [f for f in train_data.columns if f not in feature_names]\n",
    "print('Features:', feature_names, '\\nLength of Features:', len(feature_names))\n",
    "print('Targets:', target_names, '\\nLength of Features:', len(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More Dataset Viewing\n",
    "train_data['target'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Networks.NumeraiPredictionModels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m### Initialize the Neural Networks\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mNetworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mNumeraiPredictionModels\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m \n\u001b[1;32m      3\u001b[0m \u001b[39m# Testing Network\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_size \u001b[39m=\u001b[39m \u001b[39m313\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Networks.NumeraiPredictionModels'"
     ]
    }
   ],
   "source": [
    "### Initialize the Neural Networks\n",
    "from Networks.NumeraiPredictionModels import * \n",
    "# Testing Network\n",
    "input_size = 313\n",
    "batch_size = 3\n",
    "expert_decoder = ExpertDecoder(num_experts=8, num_residuals=8)\n",
    "print(f'Model\\'s Parameter Count w/ {expert_decoder.num_experts} Experts and {expert_decoder.num_residuals} Residuals Each:',sum(p.numel() for p in expert_decoder.parameters()))\n",
    "expert_decoder.eval()\n",
    "x = th.rand(size=(batch_size, input_size))\n",
    "y = expert_decoder(x)\n",
    "print(y)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
