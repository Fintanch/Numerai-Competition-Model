{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load secrets from file\n",
    "import os\n",
    "FP_Secrets = 'Numerai.secrets'\n",
    "\n",
    "if not os.path.exists(FP_Secrets):\n",
    "    raise FileNotFoundError(f\"'{FP_Secrets}' not found. Make sure the file exists.\")\n",
    "\n",
    "# Read API keys \n",
    "api_keys = {}\n",
    "with open(FP_Secrets, 'r') as secrets_file:\n",
    "    for line in secrets_file:\n",
    "        key, value = line.strip().split('=')\n",
    "        api_keys[key] = value\n",
    "\n",
    "# Set your Numerai API credentials\n",
    "PUBLIC_KEY = api_keys.get('PUBLIC_KEY')\n",
    "SECRET_KEY = api_keys.get('SECRET_KEY')\n",
    "\n",
    "if not PUBLIC_KEY or not SECRET_KEY:\n",
    "    raise ValueError(\"API keys not found in the 'numerai.secrets' file.\")\n",
    "\n",
    "import numerapi\n",
    "\n",
    "# Set your Numerai API credentials\n",
    "napi = numerapi.NumerAPI(public_id=PUBLIC_KEY, secret_key=SECRET_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the latest Numerai datasets\n",
    "napi.download_dataset(\"v4.1/train.parquet\", \"train.parquet\")\n",
    "napi.download_dataset(\"v4.1/validation.parquet\", \"validation.parquet\")\n",
    "napi.download_dataset(\"v4.1/live.parquet\", \"live.parquet\")\n",
    "napi.download_dataset(\"v4.1/live_example_preds.parquet\", \"live_example_preds.parquet\")\n",
    "napi.download_dataset(\"v4.1/validation_example_preds.parquet\", \"validation_example_preds.parquet\")\n",
    "napi.download_dataset(\"v4.1/features.json\", \"features.json\")\n",
    "napi.download_dataset(\"v4.1/meta_model.parquet\", \"meta_model.parquet\")\n",
    "\n",
    "# Challenge: How might you use the additional files like 'features.json' and 'meta_model.parquet' in your ML models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pandas DataFrames using `pd.read_parquet`\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_parquet(\"train.parquet\")\n",
    "validation_data = pd.read_parquet(\"validation.parquet\")\n",
    "live_data = pd.read_parquet(\"live.parquet\")\n",
    "live_example_preds = pd.read_parquet(\"live_example_preds.parquet\")\n",
    "validation_example_preds = pd.read_parquet(\"validation_example_preds.parquet\")\n",
    "\n",
    "# Display basic info about the data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", validation_data.shape)\n",
    "print(\"Live data shape:\", live_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializes Numerai Data and NumerAPI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numerapi\n",
    "import re\n",
    "\n",
    "# Set your Numerai API credentials\n",
    "napi = numerapi.NumerAPI(public_id=PUBLIC_KEY, secret_key=SECRET_KEY)\n",
    "\n",
    "# Download the latest Numerai dataset\n",
    "# napi.download_current_dataset(unzip=True)\n",
    "\n",
    "f_pattern = r\"numerai_dataset_\\d+\"\n",
    "f_name = None\n",
    "print(os.listdir())\n",
    "for file in os.listdir():\n",
    "    if re.match(f_pattern, file):\n",
    "        f_name = file\n",
    "        break\n",
    "\n",
    "assert f_name != None\n",
    "f_name = f_name.replace('.zip', '') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads data by chunks (My laptop does not have enough RAM)\n",
    "t_data = os.path.join(f_name, \"numerai_training_data.csv\")\n",
    "tor_data = os.path.join(f_name, \"numerai_tournament_data.csv\")\n",
    "chunk_size = 50000 \n",
    "num_chunks = 10\n",
    "chunks = []\n",
    "for i, chunk in enumerate(pd.read_csv(t_data, chunksize=chunk_size)):\n",
    "    chunks.append(chunk)\n",
    "    if i > num_chunks: break\n",
    "train_data = pd.concat(chunks, axis=0)\n",
    "\n",
    "chunks = []\n",
    "for i, chunk in enumerate(pd.read_csv(tor_data, chunksize=chunk_size)):\n",
    "    chunks.append(chunk)\n",
    "    if i > num_chunks: break\n",
    "tournament_data = pd.concat(chunks, axis=0)\n",
    "\n",
    "# Display basic info about the data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Tournament data shape:\", tournament_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## View Dataset\n",
    "feature_names = [\n",
    "        f for f in train_data.columns if f.startswith(\"feature\")\n",
    "    ]\n",
    "target_names = [f for f in train_data.columns if f not in feature_names]\n",
    "print('Features:', feature_names, '\\nLength of Features:', len(feature_names))\n",
    "print('Targets:', target_names, '\\nLength of Features:', len(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More Dataset Viewing\n",
    "train_data['target'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Parameter Count w/ 8 Experts and 8 Residuals Each: 212051585\n",
      "tensor([[0.4811],\n",
      "        [0.4800],\n",
      "        [0.4790]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Initialize the Neural Networks\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\"\"\"\n",
    "let x = Features Batch\n",
    "x -> let opinions = {ResidualFeatureEncoder_i(x) for ResidualFeatureEncoder in self.Experts}\n",
    " -> let consensus = Sum(opinions) -> Decoder(consensus) \\eq y (Target)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, residual_dim=512, brodcast_dim=1048, dropout_prob=0.1, activation_fnc=nn.GELU, device='cuda'):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "        self.residual_block = nn.Sequential(\n",
    "            nn.Linear(residual_dim, brodcast_dim),\n",
    "            activation_fnc(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(brodcast_dim, brodcast_dim),\n",
    "            activation_fnc(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(brodcast_dim, brodcast_dim),\n",
    "            activation_fnc(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(brodcast_dim, residual_dim),\n",
    "            activation_fnc(), # Needs Batch Norm\n",
    "        )\n",
    "        self._norm = nn.LayerNorm(residual_dim)\n",
    "    def forward(self, x):\n",
    "        return self._norm(x + self.residual_block(x))\n",
    "    \n",
    "class ResidualFeatureEncoder(nn.Module):\n",
    "    def __init__(self, expert_num, num_residuals=5, input_dim=313, output_dim= 256, residual_dim=512, dataset_name=\"Dataset_563\", device='cuda'):\n",
    "        super(ResidualFeatureEncoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "        self.brodcast = nn.Sequential(\n",
    "            nn.Linear(input_dim, residual_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.residuals = [ResidualBlock(residual_dim=residual_dim,) for _ in range(num_residuals)]\n",
    "        self.residuals = nn.Sequential(*self.residuals)\n",
    "        self.outcast = nn.Linear(residual_dim, output_dim)\n",
    "\n",
    "        self._expert_num = expert_num\n",
    "        self._ds_name = dataset_name\n",
    "        self.save_path = os.path.join('.', f'Networks','Checkpoints','NumeraiExperts', f\"{self._expert_num}\")\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.brodcast(x)\n",
    "        x = self.residuals(x)\n",
    "        x = self.outcast(x)\n",
    "        return x\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        if os.path.exists(os.path.join(self.save_path, f'{self._ds_name}_{self._expert_num}_weights.pt')):\n",
    "            self.load_state_dict(th.load(os.path.join(self.save_path, f'{self._ds_name}_{self._expert_num}_weights.pt')))\n",
    "\n",
    "    \n",
    "    def save_checkpoint(self, file=None):\n",
    "        print(f'[Expert {self._expert_num}] Saving Checkpoint...')\n",
    "        if file != None:\n",
    "            th.save(self.state_dict(), file)\n",
    "        else:\n",
    "            th.save(self.state_dict(), self.save_path + \"/\" + f'{self._ds_name}_{self._expert_num}_weights.pt') \n",
    "\n",
    "class ExpertDecoder(nn.Module):\n",
    "    def __init__(self, num_experts = 15, num_residuals=5, input_dim=313, output_dim=1, residual_input_dim=313, residual_output_dim= 256, residual_dim=512, dropout_probs=0.05, load_default=False, dataset_name=\"Dataset_563\", device='cuda'):\n",
    "        super(ExpertDecoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "        self.num_experts, self.num_residuals = num_experts, num_residuals\n",
    "        self.experts = nn.ModuleList([ResidualFeatureEncoder(expert_num=expert_num, num_residuals=num_residuals, input_dim=residual_input_dim, output_dim=residual_output_dim, residual_dim=residual_dim) for expert_num in range(num_experts)])\n",
    "        self._norm = nn.LayerNorm(residual_output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(residual_output_dim, residual_output_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_probs),\n",
    "            nn.Linear(residual_output_dim, residual_output_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_probs),\n",
    "            nn.Linear(residual_output_dim, residual_output_dim//4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(residual_output_dim//4, residual_output_dim//8),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(residual_output_dim//8, output_dim), # output \n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self._ds_name = dataset_name\n",
    "        self.save_path = os.path.join('.', f'Networks','Checkpoints','NumeraiExperts', 'ExpertDecoder')\n",
    "        self._file_name = f'expert_decoder_{self.num_experts}#Experts_{self.num_residuals}#Residuals_{self._ds_name}.pt'\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "    \n",
    "        elif load_default:\n",
    "            self.load_checkpoint()\n",
    "            # for expert in self.experts:\n",
    "            #     expert.load_checkpoint()\n",
    "\n",
    "    def forward(self, x):\n",
    "        expert_opinions = [expert(x) for expert in self.experts]\n",
    "        expert_opinions_tnsr = th.stack(expert_opinions, dim=0)\n",
    "        expert_consensus = th.sum(expert_opinions_tnsr, dim=0)\n",
    "        normalized_consensus = self._norm(expert_consensus)\n",
    "        y = self.decoder(normalized_consensus)\n",
    "        return y\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        if os.path.exists(os.path.join(self.save_path, self.file_name)):\n",
    "            self.load_state_dict(th.load(os.path.join(self.save_path, self.file_name)))\n",
    "    \n",
    "    def save_checkpoint(self, file=None):\n",
    "        print(f'[Expert {self._expert_num}] Saving Checkpoint...')\n",
    "        if file != None:\n",
    "            th.save(self.state_dict(), file)\n",
    "        else:\n",
    "            th.save(self.state_dict(), self.save_path + \"/\" + self.file_name)\n",
    "\n",
    "# Testing Network\n",
    "input_size = 313\n",
    "batch_size = 3\n",
    "expert_decoder = ExpertDecoder(num_experts=8, num_residuals=8)\n",
    "print(f'Model\\'s Parameter Count w/ {expert_decoder.num_experts} Experts and {expert_decoder.num_residuals} Residuals Each:',sum(p.numel() for p in expert_decoder.parameters()))\n",
    "expert_decoder.eval()\n",
    "x = th.rand(size=(batch_size, input_size))\n",
    "y = expert_decoder(x)\n",
    "print(y)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
