{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load secrets from file\n",
    "import os\n",
    "FP_Secrets = 'Numerai.secrets'\n",
    "\n",
    "if not os.path.exists(FP_Secrets):\n",
    "    raise FileNotFoundError(f\"'{FP_Secrets}' not found. Make sure the file exists.\")\n",
    "\n",
    "# Read API keys \n",
    "api_keys = {}\n",
    "with open(FP_Secrets, 'r') as secrets_file:\n",
    "    for line in secrets_file:\n",
    "        key, value = line.strip().split('=')\n",
    "        api_keys[key] = value\n",
    "\n",
    "# Set your Numerai API credentials\n",
    "PUBLIC_KEY = api_keys.get('PUBLIC_KEY')\n",
    "SECRET_KEY = api_keys.get('SECRET_KEY')\n",
    "\n",
    "if not PUBLIC_KEY or not SECRET_KEY:\n",
    "    raise ValueError(\"API keys not found in the 'numerai.secrets' file.\")\n",
    "\n",
    "import numerapi\n",
    "\n",
    "# Set your Numerai API credentials\n",
    "napi = numerapi.NumerAPI(public_id=PUBLIC_KEY, secret_key=SECRET_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the latest Numerai datasets\n",
    "napi.download_dataset(\"v4.1/train.parquet\", \"train.parquet\")\n",
    "napi.download_dataset(\"v4.1/validation.parquet\", \"validation.parquet\")\n",
    "napi.download_dataset(\"v4.1/live.parquet\", \"live.parquet\")\n",
    "napi.download_dataset(\"v4.1/live_example_preds.parquet\", \"live_example_preds.parquet\")\n",
    "napi.download_dataset(\"v4.1/validation_example_preds.parquet\", \"validation_example_preds.parquet\")\n",
    "napi.download_dataset(\"v4.1/features.json\", \"features.json\")\n",
    "napi.download_dataset(\"v4.1/meta_model.parquet\", \"meta_model.parquet\")\n",
    "\n",
    "# Challenge: How might you use the additional files like 'features.json' and 'meta_model.parquet' in your ML models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pandas DataFrames using `pd.read_parquet`\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_parquet(\"train.parquet\")\n",
    "validation_data = pd.read_parquet(\"validation.parquet\")\n",
    "live_data = pd.read_parquet(\"live.parquet\")\n",
    "live_example_preds = pd.read_parquet(\"live_example_preds.parquet\")\n",
    "validation_example_preds = pd.read_parquet(\"validation_example_preds.parquet\")\n",
    "\n",
    "# Display basic info about the data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", validation_data.shape)\n",
    "print(\"Live data shape:\", live_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializes Numerai Data and NumerAPI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numerapi\n",
    "import re\n",
    "\n",
    "# Set your Numerai API credentials\n",
    "napi = numerapi.NumerAPI(public_id=PUBLIC_KEY, secret_key=SECRET_KEY)\n",
    "\n",
    "# Download the latest Numerai dataset\n",
    "# napi.download_current_dataset(unzip=True)\n",
    "\n",
    "f_pattern = r\"numerai_dataset_\\d+\"\n",
    "f_name = None\n",
    "print(os.listdir())\n",
    "for file in os.listdir():\n",
    "    if re.match(f_pattern, file):\n",
    "        f_name = file\n",
    "        break\n",
    "\n",
    "assert f_name != None\n",
    "f_name = f_name.replace('.zip', '') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads data by chunks (My laptop does not have enough RAM)\n",
    "t_data = os.path.join(f_name, \"numerai_training_data.csv\")\n",
    "tor_data = os.path.join(f_name, \"numerai_tournament_data.csv\")\n",
    "chunk_size = 50000 \n",
    "num_chunks = 10\n",
    "chunks = []\n",
    "for i, chunk in enumerate(pd.read_csv(t_data, chunksize=chunk_size)):\n",
    "    chunks.append(chunk)\n",
    "    if i > num_chunks: break\n",
    "train_data = pd.concat(chunks, axis=0)\n",
    "\n",
    "chunks = []\n",
    "for i, chunk in enumerate(pd.read_csv(tor_data, chunksize=chunk_size)):\n",
    "    chunks.append(chunk)\n",
    "    if i > num_chunks: break\n",
    "tournament_data = pd.concat(chunks, axis=0)\n",
    "\n",
    "# Display basic info about the data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Tournament data shape:\", tournament_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in train_data['target']:\n",
    "    if element not in [0, 0.25, 0.5, 0.75, 1]: print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## View Dataset\n",
    "feature_names = [\n",
    "        f for f in train_data.columns if f.startswith(\"feature\")\n",
    "    ]\n",
    "target_names = [f for f in train_data.columns if f not in feature_names]\n",
    "print('Features:', feature_names, '\\nLength of Features:', len(feature_names))\n",
    "print('Targets:', target_names, '\\nLength of Features:', len(target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More Dataset Viewing # Num Features = len(feature_names)\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def augment_dataset(data):\n",
    "    data_points = []\n",
    "    y_outputs = F.one_hot(th.tensor((data['target'].to_numpy().reshape(-1, 1) * 4).astype(int)), num_classes=5).squeeze(dim=1)\n",
    "    for features in [f for f in data.columns if f.startswith(\"feature\")]:\n",
    "        feature = data[features].to_numpy().reshape(-1, 1)\n",
    "        data_points.append(th.tensor(feature))\n",
    "    return th.cat(data_points, dim=1), y_outputs\n",
    "\n",
    "dataset = augment_dataset(train_data)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0].shape)\n",
    "print(dataset[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "class NumerAIdataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "numerai_dataset = NumerAIdataset(*dataset)\n",
    "\n",
    "numerai_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "validation_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(dataset,[train_size, validation_size])\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, )\n",
    "validation_data_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Object Transformer!!\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "def train_model(\n",
    "    model, train_loader, val_loader,\n",
    "    batch_size = 16, \n",
    "    epochs = 1000, \n",
    "    learning_rate = 5e-3, \n",
    "    log_interval = 50, \n",
    "    no_cuda = False, \n",
    "    seed = 1, \n",
    "    is_lstm=False,\n",
    "    patience = 10):\n",
    "\n",
    "  use_cuda = not no_cuda and th.cuda.is_available()\n",
    "  device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  print(device)\n",
    "  kwargs = {}\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  noise_level = 0.001  \n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "\n",
    "  def train(model, device, train_loader, optimizer, is_lstm=is_lstm):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for data, target in train_loader:\n",
    "        # th.cuda.empty_cache()\n",
    "        i+=1\n",
    "        # if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "        # print('data', data.shape)\n",
    "        data = {k: v.to(device).squeeze(dim=0) for k, v in data.items()}\n",
    "        target = target.to(device).squeeze(dim=0)\n",
    "        optimizer.zero_grad()\n",
    "        output_target = target\n",
    "        output_prediction = model(data)\n",
    "        loss = criterion(output_prediction, output_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_schedule()\n",
    "        total_loss+=loss.item()\n",
    "        if i % log_interval == 0:\n",
    "            try:\n",
    "                print(f'Avg Loss: {(total_loss/i+1)}%')\n",
    "                train_losses.append(total_loss/i+1)\n",
    "            except:\n",
    "                pass\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "  def validation(model, device, val_loader, is_lstm=is_lstm):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    with th.no_grad():\n",
    "      for data, target in val_loader:\n",
    "        # if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "        data = {k: v.to(device).squeeze(dim=0) for k, v in data.items()}\n",
    "        target = target.to(device).squeeze(dim=0)\n",
    "        output_target = target\n",
    "        output_prediction = model(data)\n",
    "        loss = criterion(output_prediction, output_target)\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    val_loss = loss_total / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print('Validation_loss:', val_loss)\n",
    "    return val_loss\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  optimizer = optim.RMSprop(model.parameters(), lr = start_lr,\n",
    "        eps=1e-7,\n",
    "        weight_decay=0.002,\n",
    "        # momentum=0.92,\n",
    "        # centered=True\n",
    "    )\n",
    "  print('Training...')\n",
    "  for epoch in range(1, epochs+1):\n",
    "    train_loss = train(model, device, train_loader, optimizer)\n",
    "    if epoch % 10 == 0 :\n",
    "        val_loss = validation(model, device, val_loader)\n",
    "    if epoch % 50 == 0:\n",
    "        model.save_checkpoint()\n",
    "  plt.figure()\n",
    "  plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "  plt.xlabel('Seq')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.savefig('train_loss.png')\n",
    "  plt.figure()\n",
    "  plt.plot(range(len(val_losses)), val_losses, label='Val Loss')\n",
    "  plt.xlabel('Seq')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.savefig('val_loss.png')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the Neural Networks\n",
    "from Networks.NumeraiPredictionModels import * \n",
    "# Testing Network\n",
    "input_size = 310\n",
    "batch_size = 3\n",
    "expert_decoder = ExpertDecoder(num_experts=8, num_residuals=8)\n",
    "print(f'Model\\'s Parameter Count w/ {expert_decoder.num_experts} Experts and {expert_decoder.num_residuals} Residuals Each:',sum(p.numel() for p in expert_decoder.parameters()))\n",
    "expert_decoder.eval()\n",
    "x = th.rand(size=(batch_size, input_size))\n",
    "y = expert_decoder(x)\n",
    "# for tens in range(len(y)):\n",
    "#     print(y[tens],x[tens])\n",
    "print(y)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
